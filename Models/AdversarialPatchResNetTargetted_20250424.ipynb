{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab9ddcb9",
   "metadata": {},
   "source": [
    "## Advevrsarial Patch On CIFAR10 Using ResNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef5e8caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "  print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "  print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ece34b",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abebe8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Training Data Proportion: 0.83\n",
      "Test Data Proportion: 0.08\n",
      "Validation Data Proportion: 0.08\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# convert data to a normalized torch.FloatTensor\n",
    "print('==> Preparing data..')\n",
    "#Image augmentation is used to train the model\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "#Only the data is normalaized we do not need to augment the test data\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.CIFAR10('data', train=True,\n",
    "                              download=True, transform=transform_train)\n",
    "combined_test_data = datasets.CIFAR10('data', train=False,\n",
    "                             download=True, transform=transform_test)\n",
    "\n",
    "\n",
    "# Get targets from the test data for stratification\n",
    "targets = combined_test_data.targets\n",
    "\n",
    "# Stratified split for validation and test sets from the original test dataset\n",
    "sss_test_valid = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)  # Split 50/50\n",
    "valid_idx, test_idx = next(sss_test_valid.split(np.zeros(len(targets)), targets))\n",
    "\n",
    "# Convert indices to actual Subset for validation and test datasets\n",
    "valid_dataset = Subset(combined_test_data, valid_idx)\n",
    "test_dataset = Subset(combined_test_data, test_idx)\n",
    "\n",
    "# Prepare data loaders\n",
    "batch_size = 20\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Total number of samples in the complete dataset\n",
    "total_samples = len(train_loader) + len(valid_loader) + len(test_loader)\n",
    "\n",
    "# Assuming the splits have been previously calculated:\n",
    "# Sizes of each dataset split\n",
    "train_size = len(train_loader)\n",
    "test_size = len(test_loader)\n",
    "validation_size = len(valid_loader)\n",
    "\n",
    "# Calculate proportions\n",
    "train_proportion = train_size / total_samples\n",
    "test_proportion = test_size / total_samples\n",
    "validation_proportion = validation_size / total_samples\n",
    "\n",
    "# Print the proportions\n",
    "print(f\"Training Data Proportion: {train_proportion:.2f}\")\n",
    "print(f\"Test Data Proportion: {test_proportion:.2f}\")\n",
    "print(f\"Validation Data Proportion: {validation_proportion:.2f}\")\n",
    "\n",
    "\n",
    "# specify the image classes\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c45dd6",
   "metadata": {},
   "source": [
    "### Model Definiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d98042e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "  expansion = 1\n",
    "  def __init__(self, in_planes, planes, stride=1):\n",
    "    super(BasicBlock, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "    self.bn1 = nn.BatchNorm2d(planes)\n",
    "    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "    self.shortcut = nn.Sequential()\n",
    "    if stride != 1 or in_planes != self.expansion*planes:\n",
    "      self.shortcut = nn.Sequential(\n",
    "          nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "          nn.BatchNorm2d(self.expansion*planes)\n",
    "      )\n",
    "  \n",
    "  def forward(self, x):\n",
    "    out = F.relu(self.bn1(self.conv1(x)))\n",
    "    out = self.bn2(self.conv2(out))\n",
    "    out += self.shortcut(x)\n",
    "    out = F.relu(out)\n",
    "    return out\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "  expansion = 4\n",
    "\n",
    "  def __init__(self, in_planes, planes, stride=1):\n",
    "    super(BottleNeck, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(in_planes , planes, kernel_size=1, bias=False)\n",
    "    self.bn1 = nn.BatchNorm2d(planes)\n",
    "    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "    self.bn2 = nn.BatchNorm2d(planes)\n",
    "    self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "    self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "    self.shortcut = nn.Sequential()\n",
    "    if stride != 1 or in_planes != self.expansion*planes :\n",
    "      self.shortcut = nn.Sequential(\n",
    "          nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "          nn.BatchNorm2d(self.expansion*planes)\n",
    "      )\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = F.relu(self.bn1(self.conv1(x)))\n",
    "    out = F.relu(self.bn2(self.conv2(out)))\n",
    "    out = self.bn3(self.conv3(out))\n",
    "    out += self.shortcut(x)\n",
    "    out = F.relu(out)\n",
    "    return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "  def __init__(self, block, num_blocks, num_classes=10):\n",
    "    super(ResNet, self).__init__()\n",
    "    self.in_planes = 64\n",
    "\n",
    "    self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    self.bn1 = nn.BatchNorm2d(64)\n",
    "    self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "    self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "    self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "    self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "    self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "  def _make_layer(self, block, planes, num_blocks, stride):\n",
    "    strides = [stride] + [1]*(num_blocks-1)\n",
    "    layers = []\n",
    "    for stride in strides:\n",
    "      layers.append(block(self.in_planes, planes, stride))\n",
    "      self.in_planes = planes * block.expansion      \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = F.relu(self.bn1(self.conv1(x)))\n",
    "    out = self.layer1(out)\n",
    "    out = self.layer2(out)\n",
    "    out = self.layer3(out)\n",
    "    out = self.layer4(out)\n",
    "    out = F.avg_pool2d(out, 4)\n",
    "    out = out.view(out.size(0), -1)\n",
    "    out = self.linear(out)\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faabe60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet18 = ResNet(BasicBlock, [2,2,2,2])\n",
    "\n",
    "\n",
    "if train_on_gpu:\n",
    "  ResNet18 = torch.nn.DataParallel(ResNet18)\n",
    "  cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a3c5318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained ResNet18 loaded and ready!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load Pretrained model\n",
    "ResNet18.load_state_dict(torch.load('ResNet18_20250419.pt'))\n",
    "\n",
    "print(\"Pretrained ResNet18 loaded and ready!\")\n",
    "\n",
    "# Step 4: Move model to device and eval mode\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ResNet18 = ResNet18.to(device)\n",
    "ResNet18.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7078f3",
   "metadata": {},
   "source": [
    "### Adversarial Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d055a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d0d121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a random patch\n",
    "def create_patch(patch_size=(3, 3)):\n",
    "    patch = torch.randn(3, *patch_size, requires_grad=True, device=device)\n",
    "    return patch\n",
    "\n",
    "# Function to apply the patch to images\n",
    "def place_patch(img, patch):\n",
    "    batch_size, _, h, w = img.size()\n",
    "    ph, pw = patch.size(1), patch.size(2)\n",
    "    for i in range(batch_size):\n",
    "        x_offset = torch.randint(0, h - ph + 1, (1,)).item()\n",
    "        y_offset = torch.randint(0, w - pw + 1, (1,)).item()\n",
    "        img[i, :, x_offset:x_offset+ph, y_offset:y_offset+pw] = patch\n",
    "    return img\n",
    "\n",
    "# Training function for adversarial patch\n",
    "def patch_training_step(model, patch, target_class=None, dataloader=None, optimizer=None, criterion=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, _ in dataloader:\n",
    "        images = images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        patched_images = place_patch(images, patch)\n",
    "        outputs = model(patched_images)\n",
    "        if target_class is not None:\n",
    "            labels = torch.full((images.size(0),), target_class, dtype=torch.long, device=device)\n",
    "        else:\n",
    "            labels = torch.randint(0, 10, (images.size(0),), device=device)  # Random class for untargeted attack\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Function to train the adversarial patch\n",
    "def train_adversarial_patch(model, patch, target_class=None, num_epochs=10):\n",
    "    patch_optimizer = optim.Adam([patch], lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        loss = patch_training_step(model, patch, target_class, train_loader, patch_optimizer, criterion)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Function to evaluate the success rate of the adversarial patch\n",
    "def evaluate_patch(model, patch, dataloader, target_class=None):\n",
    "    model.eval()\n",
    "    success = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            patched_images = place_patch(images, patch)\n",
    "            outputs = model(patched_images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            if target_class is not None:\n",
    "                success += (predicted == target_class).sum().item()\n",
    "            else:\n",
    "                success += (predicted != labels).sum().item()  # Evaluate untargeted attack\n",
    "            total += labels.size(0)\n",
    "\n",
    "    successR = 100 * success / total\n",
    "    print(f\"Attack Success Rate: {successR:.2f}%\")\n",
    "    return successR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fa9538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for patch size: (16, 16)\n",
      "Epoch 1/20, Loss: 1.6280\n",
      "Epoch 2/20, Loss: 0.7271\n",
      "Epoch 3/20, Loss: 0.4865\n",
      "Epoch 4/20, Loss: 0.3757\n",
      "Epoch 5/20, Loss: 0.3110\n",
      "Epoch 6/20, Loss: 0.2641\n",
      "Epoch 7/20, Loss: 0.2297\n",
      "Epoch 8/20, Loss: 0.2067\n",
      "Epoch 9/20, Loss: 0.1920\n",
      "Epoch 10/20, Loss: 0.1802\n",
      "Epoch 11/20, Loss: 0.1700\n",
      "Epoch 12/20, Loss: 0.1612\n",
      "Epoch 13/20, Loss: 0.1551\n",
      "Epoch 14/20, Loss: 0.1484\n",
      "Epoch 15/20, Loss: 0.1451\n",
      "Epoch 16/20, Loss: 0.1392\n",
      "Epoch 17/20, Loss: 0.1364\n",
      "Epoch 18/20, Loss: 0.1322\n",
      "Epoch 19/20, Loss: 0.1292\n",
      "Epoch 20/20, Loss: 0.1271\n",
      "Attack Success Rate: 99.68%\n",
      "Epoch 1/20, Loss: 3.3206\n",
      "Epoch 2/20, Loss: 1.9742\n",
      "Epoch 3/20, Loss: 1.4719\n",
      "Epoch 4/20, Loss: 1.2258\n",
      "Epoch 5/20, Loss: 1.0544\n",
      "Epoch 6/20, Loss: 0.9512\n",
      "Epoch 7/20, Loss: 0.8742\n",
      "Epoch 8/20, Loss: 0.8242\n",
      "Epoch 9/20, Loss: 0.7717\n",
      "Epoch 10/20, Loss: 0.7375\n",
      "Epoch 11/20, Loss: 0.7073\n",
      "Epoch 12/20, Loss: 0.6759\n",
      "Epoch 13/20, Loss: 0.6526\n",
      "Epoch 14/20, Loss: 0.6309\n",
      "Epoch 15/20, Loss: 0.6128\n",
      "Epoch 16/20, Loss: 0.5928\n",
      "Epoch 17/20, Loss: 0.5790\n",
      "Epoch 18/20, Loss: 0.5648\n",
      "Epoch 19/20, Loss: 0.5520\n",
      "Epoch 20/20, Loss: 0.5351\n",
      "Attack Success Rate: 86.18%\n",
      "Epoch 1/20, Loss: 2.4706\n",
      "Epoch 2/20, Loss: 1.1564\n",
      "Epoch 3/20, Loss: 0.7812\n",
      "Epoch 4/20, Loss: 0.5840\n",
      "Epoch 5/20, Loss: 0.4774\n",
      "Epoch 6/20, Loss: 0.4149\n",
      "Epoch 7/20, Loss: 0.3734\n",
      "Epoch 8/20, Loss: 0.3468\n",
      "Epoch 9/20, Loss: 0.3266\n",
      "Epoch 10/20, Loss: 0.3075\n",
      "Epoch 11/20, Loss: 0.2951\n",
      "Epoch 12/20, Loss: 0.2825\n",
      "Epoch 13/20, Loss: 0.2742\n",
      "Epoch 14/20, Loss: 0.2656\n",
      "Epoch 15/20, Loss: 0.2608\n",
      "Epoch 16/20, Loss: 0.2537\n",
      "Epoch 17/20, Loss: 0.2458\n",
      "Epoch 18/20, Loss: 0.2421\n",
      "Epoch 19/20, Loss: 0.2382\n",
      "Epoch 20/20, Loss: 0.2341\n",
      "Attack Success Rate: 98.82%\n",
      "Epoch 1/20, Loss: 3.2730\n",
      "Epoch 2/20, Loss: 2.2856\n",
      "Epoch 3/20, Loss: 1.6668\n",
      "Epoch 4/20, Loss: 1.2938\n",
      "Epoch 5/20, Loss: 1.0738\n",
      "Epoch 6/20, Loss: 0.9356\n",
      "Epoch 7/20, Loss: 0.8328\n",
      "Epoch 8/20, Loss: 0.7645\n",
      "Epoch 9/20, Loss: 0.7032\n",
      "Epoch 10/20, Loss: 0.6590\n",
      "Epoch 11/20, Loss: 0.6196\n",
      "Epoch 12/20, Loss: 0.5877\n",
      "Epoch 13/20, Loss: 0.5593\n",
      "Epoch 14/20, Loss: 0.5288\n",
      "Epoch 15/20, Loss: 0.5072\n",
      "Epoch 16/20, Loss: 0.4921\n",
      "Epoch 17/20, Loss: 0.4759\n",
      "Epoch 18/20, Loss: 0.4613\n",
      "Epoch 19/20, Loss: 0.4483\n",
      "Epoch 20/20, Loss: 0.4352\n",
      "Attack Success Rate: 92.38%\n",
      "Epoch 1/20, Loss: 2.2457\n",
      "Epoch 2/20, Loss: 1.3257\n",
      "Epoch 3/20, Loss: 0.9293\n",
      "Epoch 4/20, Loss: 0.6995\n",
      "Epoch 5/20, Loss: 0.5776\n",
      "Epoch 6/20, Loss: 0.4990\n",
      "Epoch 7/20, Loss: 0.4439\n",
      "Epoch 8/20, Loss: 0.4032\n",
      "Epoch 9/20, Loss: 0.3707\n",
      "Epoch 10/20, Loss: 0.3483\n",
      "Epoch 11/20, Loss: 0.3253\n",
      "Epoch 12/20, Loss: 0.3069\n",
      "Epoch 13/20, Loss: 0.2913\n",
      "Epoch 14/20, Loss: 0.2799\n",
      "Epoch 15/20, Loss: 0.2688\n",
      "Epoch 16/20, Loss: 0.2555\n",
      "Epoch 17/20, Loss: 0.2459\n",
      "Epoch 18/20, Loss: 0.2387\n",
      "Epoch 19/20, Loss: 0.2329\n",
      "Epoch 20/20, Loss: 0.2260\n",
      "Attack Success Rate: 99.40%\n"
     ]
    }
   ],
   "source": [
    "# CIFAR-10 class names\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Different patch sizes\n",
    "patch_sizes = [(3, 3), (5, 5), (7, 7), (16, 16)]\n",
    "\n",
    "ASR_dict = {}\n",
    "\n",
    "# Train and evaluate patches of different sizes\n",
    "for patch_size in patch_sizes:\n",
    "    ASR_dict[patch_size] = list()\n",
    "    print(f\"\\nTraining for patch size: {patch_size}\")\n",
    "    for c in range(10):\n",
    "    \n",
    "        patch = create_patch(patch_size)\n",
    "        train_adversarial_patch(ResNet18, patch, target_class=c, num_epochs=20)\n",
    "        # Save the patch\n",
    "        torch.save(patch, f'adv_patch_{patch_size[0]}x{patch_size[1]}_targeted_{classes[c]}_20250424.pth')\n",
    "\n",
    "        # Evaluate attack success rate\n",
    "        ASR = evaluate_patch(ResNet18, patch, test_loader, target_class=c)\n",
    "        ASR_dict[patch_size].append(ASR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3e1b9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating targeted adversarial patch, size: (3, 3), target class: plane\n",
      "Attack Success Rate: 53.26%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (3, 3), target class: car\n",
      "Attack Success Rate: 0.00%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (3, 3), target class: bird\n",
      "Attack Success Rate: 6.76%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (3, 3), target class: cat\n",
      "Attack Success Rate: 17.72%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (3, 3), target class: deer\n",
      "Attack Success Rate: 0.00%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (3, 3), target class: dog\n",
      "Attack Success Rate: 0.18%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (3, 3), target class: frog\n",
      "Attack Success Rate: 0.08%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (3, 3), target class: horse\n",
      "Attack Success Rate: 0.00%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (3, 3), target class: ship\n",
      "Attack Success Rate: 6.78%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (3, 3), target class: truck\n",
      "Attack Success Rate: 0.00%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (5, 5), target class: plane\n",
      "Attack Success Rate: 81.48%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (5, 5), target class: car\n",
      "Attack Success Rate: 2.50%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (5, 5), target class: bird\n",
      "Attack Success Rate: 77.06%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (5, 5), target class: cat\n",
      "Attack Success Rate: 4.48%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (5, 5), target class: deer\n",
      "Attack Success Rate: 0.00%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (5, 5), target class: dog\n",
      "Attack Success Rate: 1.46%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (5, 5), target class: frog\n",
      "Attack Success Rate: 8.44%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (5, 5), target class: horse\n",
      "Attack Success Rate: 0.00%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (5, 5), target class: ship\n",
      "Attack Success Rate: 22.24%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (5, 5), target class: truck\n",
      "Attack Success Rate: 0.00%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (7, 7), target class: plane\n",
      "Attack Success Rate: 54.90%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (7, 7), target class: car\n",
      "Attack Success Rate: 23.74%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (7, 7), target class: bird\n",
      "Attack Success Rate: 92.30%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (7, 7), target class: cat\n",
      "Attack Success Rate: 100.00%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (7, 7), target class: deer\n",
      "Attack Success Rate: 74.32%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (7, 7), target class: dog\n",
      "Attack Success Rate: 38.98%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (7, 7), target class: frog\n",
      "Attack Success Rate: 77.72%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (7, 7), target class: horse\n",
      "Attack Success Rate: 0.00%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (7, 7), target class: ship\n",
      "Attack Success Rate: 43.44%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (7, 7), target class: truck\n",
      "Attack Success Rate: 0.00%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (16, 16), target class: plane\n",
      "Attack Success Rate: 100.00%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (16, 16), target class: car\n",
      "Attack Success Rate: 20.92%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (16, 16), target class: bird\n",
      "Attack Success Rate: 100.00%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (16, 16), target class: cat\n",
      "Attack Success Rate: 100.00%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (16, 16), target class: deer\n",
      "Attack Success Rate: 100.00%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (16, 16), target class: dog\n",
      "Attack Success Rate: 100.00%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (16, 16), target class: frog\n",
      "Attack Success Rate: 38.56%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (16, 16), target class: horse\n",
      "Attack Success Rate: 0.00%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (16, 16), target class: ship\n",
      "Attack Success Rate: 82.24%\n",
      "\n",
      "Evaluating targeted adversarial patch, size: (16, 16), target class: truck\n",
      "Attack Success Rate: 99.42%\n"
     ]
    }
   ],
   "source": [
    "# CIFAR-10 class names\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Different patch sizes\n",
    "patch_sizes = [(3, 3), (5, 5), (7, 7), (16, 16)]\n",
    "ASR_dict = {}\n",
    "# Evaluate patches of different sizes\n",
    "for patch_size in patch_sizes:\n",
    "    ASR_dict[patch_size] = list()\n",
    "    for c in range(10):\n",
    "        print(f\"\\nEvaluating targeted adversarial patch, size: {patch_size}, target class: {classes[c]}\")\n",
    "        patch = create_patch(patch_size)\n",
    "        patch = torch.load(f'adv_patch_{patch_size[0]}x{patch_size[1]}_targeted_{classes[c]}_20250424.pth')\n",
    "        # Evaluate attack success rate        \n",
    "        ASR = evaluate_patch(ResNet18, patch, test_loader, target_class=c)\n",
    "        ASR_dict[patch_size].append(ASR)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f49d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary_list = []\n",
    "for s in ASR_dict:\n",
    "    Summary_list.append([s]+ ASR_dict[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fd68994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Summary = pd.DataFrame(Summary_list, columns=['Patch Size'] + classes)\n",
    "Summary.to_csv('ASR_summary_targeted_20250425.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
